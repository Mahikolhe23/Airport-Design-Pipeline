# **Airport Data Pipeline**

## **ğŸ“Œ Overview**
The **Airport Data Pipeline** project is designed to ingest, process, and store airport-related data efficiently. It fetches raw data from multiple sources (e.g., APIs, CSV files), transforms it using ETL scripts, and loads it into a structured database. The processed data can then be used for analytics and reporting.

## **ğŸ“‚ Project Structure**
```
ğŸ“‚ Airport-Data-Pipeline
â”‚â”€â”€ ğŸ“‚ datasets         # Raw and processed data
â”‚â”€â”€ ğŸ“‚ sql_scripts      # SQL scripts for table creation and data processing
â”‚â”€â”€ ğŸ“‚ etl_scripts      # Python scripts for ETL pipeline
â”‚â”€â”€ ğŸ“œ README.md        # Project overview and documentation
â”‚â”€â”€ ğŸ“œ requirements.txt # Python dependencies
```

## **ğŸ”¹ Features**
âœ… Fetches data from external sources (APIs, CSVs)  
âœ… Cleans and transforms data using **Python & Pandas**  
âœ… Loads structured data into **SQL Server**  
âœ… Automates the ETL pipeline  

---
## **âš™ï¸ Technologies Used**
- **Python** (ETL scripting)
- **Pandas** (Data manipulation)
- **SQL Server** (Database)
- **SQLAlchemy** (Database connection)
- **ODBC Driver 18** (For SQL Server connectivity)
- **Azure Data Studio** (SQL management)

---
## **ğŸ“¦ Setup & Installation**
### **1ï¸âƒ£ Install Dependencies**
Make sure you have Python installed, then install required libraries:
```bash
pip install -r requirements.txt
```

### **2ï¸âƒ£ Setup SQL Server**
Ensure SQL Server is running, and create the database:
```sql
CREATE DATABASE airport_db;
```
Run the **SQL scripts** from `sql_scripts/` to create tables.

### **3ï¸âƒ£ Configure Database Connection**
Edit the `config.py` file (if available) with your SQL Server details:
```python
DB_CONFIG = {
    'server': 'localhost,1433',
    'database': 'airport_db',
    'user': 'SA',
    'password': 'YourPassword',
    'driver': 'ODBC Driver 18 for SQL Server'
}
```

---
## **ğŸ”„ ETL Pipeline Execution**
### **1ï¸âƒ£ Fetch Data from API / CSV**
```bash
python etl_scripts/extract.py
```

### **2ï¸âƒ£ Transform Data**
```bash
python etl_scripts/transform.py
```

### **3ï¸âƒ£ Load Data into SQL Server**
```bash
python etl_scripts/load.py
```

Or run the entire ETL process in one step:
```bash
python etl_scripts/main.py
```

---
## **ğŸ“Š Data Sources**
- **FAA Passenger Data:** [FAA Website](https://www.faa.gov/airports/planning_capacity/passenger_allcargo_stats/)
- **Kaggle Datasets:** [Kaggle](https://www.kaggle.com/)

---
## **ğŸš€ Future Improvements**
- Implement **Airflow** for ETL scheduling
- Optimize SQL queries for faster execution
- Deploy on **Cloud (Azure/AWS)**

---
## **ğŸ“œ License**
This project is **open-source** and free to use.

### **ğŸ‘¨â€ğŸ’» Author: Mahendra Kolhe**

